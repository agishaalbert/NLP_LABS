{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agishaalbert/NLP_LABS/blob/main/AlbertAgisha_Lab2_intro_to_wordvectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XR1-RWJfaSS-"
      },
      "source": [
        "\n",
        "<h1 style=\"font-family:verdana;font-size:300%;text-align:center;background-color:#f2f2f2;color:#0d0d0d\">AMMI NLP - Review sessions</h1>\n",
        "\n",
        "<h1 style=\"font-family:verdana;font-size:180%;text-align:Center;color:#993333\"> Lab 2: Introduction to wordvectors </h1>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfPTitFNaSTA"
      },
      "source": [
        "**Big thanks to Amr Khalifa who improved this lab and made it to a Jupyter Notebook!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "KfMVEL8uaSTA"
      },
      "outputs": [],
      "source": [
        "import io, sys\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eMoPoiXLMq4E"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "txHkDOohaSTB"
      },
      "outputs": [],
      "source": [
        "def load_vectors(filename):\n",
        "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
        "    n, d = map(int, fin.readline().split())\n",
        "    data = {}\n",
        "    for line in fin:\n",
        "        tokens = line.rstrip().split(' ')\n",
        "        data[tokens[0]] = np.asarray([float(x) for x in tokens[1:]])\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cl-Lz6bmNiAc"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "bB6SrlTBaSTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c271f5a-407a-45c8-cdf4-1ea7a8c6d2f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ** Word vectors ** \n",
            "\n",
            "<class 'numpy.ndarray'> 300\n"
          ]
        }
      ],
      "source": [
        "# Loading word vectors\n",
        "\n",
        "print('')\n",
        "print(' ** Word vectors ** ')\n",
        "print('')\n",
        "\n",
        "'''\n",
        "word_vectors is a dictionary that maps words to their numerical word vector\n",
        "[word (string)] = [np-array] \n",
        "'''\n",
        "word_vectors = load_vectors('wiki.en.vec')\n",
        "\n",
        "tree_vector = word_vectors['tree']\n",
        "print(type(tree_vector), len(tree_vector))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "K87a1gp8aSTB"
      },
      "outputs": [],
      "source": [
        "## This function computes the cosine similarity between vectors u and v\n",
        "\n",
        "def cosine(u, v):\n",
        "    '''\n",
        "    Parameters:\n",
        "    u : 1-D numpy array\n",
        "    v : 1-D numpy array \n",
        "    \n",
        "    Returns:\n",
        "    cos (float) : value of the cosine similairy between vectors u, v \n",
        "    '''\n",
        "\n",
        "    ## FILL CODE\n",
        "    distance = 0.0\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Compute the dot product between u and v (≈1 line)\n",
        "    dot = np.dot(u,v)\n",
        "    # Compute the L2 norm of u (≈1 line)\n",
        "    norm_u = np.sqrt(np.sum(u * u))\n",
        "    \n",
        "    # Compute the L2 norm of v (≈1 line)\n",
        "    norm_v = np.sqrt(np.sum(v * v))\n",
        "    # Compute the cosine similarity defined by formula (1) (≈1 line)\n",
        "    cosine_similarity = dot / (norm_u * norm_v)\n",
        "    ### END CODE HERE ###\n",
        "    return cosine_similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "Ma7asnr4aSTC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bcf8191-0742-4496-ca81-dce3da5d3c9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "similarity(apple, apples) = 0.637\n",
            "similarity(apple, banana) = 0.431\n",
            "similarity(apple, tiger) = 0.212\n"
          ]
        }
      ],
      "source": [
        "# compute similarity between words\n",
        "\n",
        "print('similarity(apple, apples) = %.3f' %\n",
        "      cosine(word_vectors['apple'], word_vectors['apples']))\n",
        "print('similarity(apple, banana) = %.3f' %\n",
        "      cosine(word_vectors['apple'], word_vectors['banana']))\n",
        "print('similarity(apple, tiger) = %.3f' %\n",
        "      cosine(word_vectors['apple'], word_vectors['tiger']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "DPI_S-zmaSTC"
      },
      "outputs": [],
      "source": [
        "## Functions for nearest neighbor\n",
        "## This function returns the word corresponding to \n",
        "## nearest neighbor vector of x\n",
        "## The list exclude_words can be used to exclude some\n",
        "## words from the nearest neighbors search\n",
        "\n",
        "def nearest_neighbor(x, word_vectors, exclude_words=[]):\n",
        "    '''\n",
        "    Parameters:\n",
        "    x (string): word to find its nearest neighbour \n",
        "    word_vectors (Python dict): {word (string): np-array of word vector}\n",
        "    exclude_words (list of strings): words to be excluded from the search\n",
        "    \n",
        "    Returns:\n",
        "    best_word (string) : the word whose word vector is the nearest neighbour \n",
        "    to the word vector of x\n",
        "    '''\n",
        "    best_score = -1.0\n",
        "    best_word = None\n",
        "\n",
        "    ## FILL CODE\n",
        "    \n",
        "    key_list = list(word_vectors.keys())\n",
        "    liscos=[]#compute the similarity(list of sim)\n",
        "    for word in  word_vectors:\n",
        "      if word not in exclude_words:\n",
        "        liscos.append(cosine(x,word_vectors[word]))\n",
        "\n",
        "    nearn=np.argmax(liscos)\n",
        "    best_word=key_list[nearn]\n",
        "            \n",
        "    return best_word"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbor(word_vectors['cat'], word_vectors, exclude_words = ['cat','cats'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KzFr2dMVoaGz",
        "outputId": "3616429d-1b17-470d-8b07-7fdfc781f873"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dog'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8bJDa7AlZgMs"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check"
      ],
      "metadata": {
        "id": "9iqnKUZ5ZhZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# type(word_vectors)\n",
        "# key_list=list(word_vectors.keys())\n",
        "# key_list[1]\n",
        "\n",
        "list(word_vectors.keys())[4938]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6TXmephGibRu",
        "outputId": "e23d56fa-3b4d-49ca-a1d9-ff4c04025cca"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'#aaa'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check"
      ],
      "metadata": {
        "id": "wda9Jf3iZmz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# liscos=[]\n",
        "# for word in  word_vectors:\n",
        "#   if word not in ['cat','cats']:\n",
        "#     liscos.append(cosine(word_vectors['cat'],word_vectors[word]))\n",
        "\n",
        "# np.argmax(liscos),list(word_vectors.keys())[np.argmax(liscos)]"
      ],
      "metadata": {
        "id": "GIEwC8Toiqpn"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "BQOM2mMOaSTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a55ab2e-4cad-4a50-9b89-27bc59ed0983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The nearest neighbor of cat is: dog\n"
          ]
        }
      ],
      "source": [
        "print('')\n",
        "print('The nearest neighbor of cat is: ' +\n",
        "      nearest_neighbor(word_vectors['cat'], word_vectors, exclude_words = ['cat', 'cats']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CDkxOacCvBP5"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XhpCMyzaSTD"
      },
      "source": [
        "#### Hint (using python priorty queues with the heapq datastructure): \n",
        "if you don't want to store all the words and scores you can use the priortiy queue and only store the best K element so far. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "I3bc-GulaSTD"
      },
      "outputs": [],
      "source": [
        "\n",
        "## This function return the words corresponding to the\n",
        "## K nearest neighbors of vector x.\n",
        "## You can use the functions heappush and heappop.\n",
        "\n",
        "def knn(x, vectors, k):\n",
        "    '''\n",
        "    Parameters:\n",
        "    x (string): word to find its nearest neighbour \n",
        "    word_vectors (Python dict): {word (string): np-array of word vector}\n",
        "    k (int): number of nearest neighbours to be found\n",
        "    \n",
        "    Returns: \n",
        "    k_nearest_neighbors (list of tuples): [(score, word), (score, word), ....]\n",
        "    '''\n",
        "\n",
        "    k_nearest_neighbors =[]\n",
        "    ## FILL CODE\n",
        "    key_list = list(vectors.keys())\n",
        "    liscos=[]#compute the similarity(list of sim)\n",
        "    for word in  vectors:\n",
        "      liscos.append(cosine(x,vectors[word]))\n",
        "\n",
        "    liscosort=np.argsort(liscos)\n",
        "    largest_indices = liscosort[::-1][:k]\n",
        "    for i in largest_indices:\n",
        "      k_nearest_neighbors.append((liscos[i],key_list[i]))\n",
        "   \n",
        "    return k_nearest_neighbors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Sbp4qE9F3hPZ"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check"
      ],
      "metadata": {
        "id": "QVDSIGxiZuJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# liscos=[]\n",
        "# for word in  word_vectors:\n",
        "#   if word not in ['cat','cats']:\n",
        "#     liscos.append(cosine(word_vectors['cat'],word_vectors[word]))\n",
        "\n",
        "# np.argsort(liscos),list(word_vectors.keys())[np.argmax(liscos)]"
      ],
      "metadata": {
        "id": "eCv2QjM6kLeB"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "check"
      ],
      "metadata": {
        "id": "MZofcDp-Zvu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# np.argsort(liscos)[0:5]"
      ],
      "metadata": {
        "id": "xzsGoIGo4Bnv"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list(word_vectors.keys())[9453]"
      ],
      "metadata": {
        "id": "IWqQ9B9i5PSx"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "fPtl9GNJaSTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee7ef3b5-5b0a-475b-a5d5-bd91748633aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "cat\n",
            "--------------\n",
            "cat\t1.000\n",
            "cats\t0.732\n",
            "dog\t0.638\n",
            "pet\t0.573\n",
            "rabbit\t0.549\n"
          ]
        }
      ],
      "source": [
        "knn_cat = knn(word_vectors['cat'], word_vectors, 5)\n",
        "print('')\n",
        "print('cat')\n",
        "print('--------------')\n",
        "for score, word in knn(word_vectors['cat'], word_vectors, 5):\n",
        "    print (word + '\\t%.3f' % score)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGsrydB1aSTE"
      },
      "source": [
        "#### Hint: \n",
        "To find the analogies, we find the nearest neighbour associated with the wordvector d\n",
        "$$ d = \\frac{c}{\\Vert {c} \\Vert} + \\frac{b}{\\Vert {b} \\Vert} - \\frac{a}{\\Vert {a} \\Vert}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "mMaa9ZETaSTE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This function return the words d, such that a:b and c:d\n",
        "## verifies the same relation\n",
        "def analogy(a, b, c, word_vectors):\n",
        "    '''\n",
        "    Parameters:\n",
        "    a (string): word a\n",
        "    b (string): word b\n",
        "    c (string): word c\n",
        "    word_vectors (Python dict): {word (string): np-array of word vector}\n",
        "    \n",
        "    Returnrs: \n",
        "    the word d (string) associated with c such that c:d is similar to a:b \n",
        "    \n",
        "    '''\n",
        "    \n",
        "    ## FILL CODE\n",
        "    \n",
        "    # convert words to lower case\n",
        "    a, b, c = a.lower(), b.lower(), c.lower()\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    # Get the word embeddings v_a, v_b and v_c (≈1-3 lines)\n",
        "    e_a, e_b, e_c = word_vectors[a], word_vectors[b], word_vectors[c]\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    words = word_vectors.keys()\n",
        "    max_cosine_sim = -100              # Initialize max_cosine_sim to a large negative number\n",
        "    best_word = None                   # Initialize best_word with None, it will help keep track of the word to output\n",
        "\n",
        "    # loop over the whole word vector set\n",
        "    for w in words:        \n",
        "        # to avoid best_word being one of the input words, pass on them.\n",
        "        if w in [a, b, c] :\n",
        "            continue\n",
        "        \n",
        "        ### START CODE HERE ###\n",
        "        # Compute cosine similarity between the vector (e_b - e_a) and the vector ((w's vector representation) - e_c)  (≈1 line)\n",
        "        cosine_sim = cosine(e_b - e_a, word_vectors[w] - e_c)\n",
        "        \n",
        "        # If the cosine_sim is more than the max_cosine_sim seen so far,\n",
        "            # then: set the new max_cosine_sim to the current cosine_sim and the best_word to the current word (≈3 lines)\n",
        "        if cosine_sim > max_cosine_sim:\n",
        "            max_cosine_sim = cosine_sim\n",
        "            best_word = w\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "    return best_word\n"
      ],
      "metadata": {
        "id": "pU2XeZ6ak0Jy"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "nTsGDDBHaSTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47a123ee-727e-4cea-f95b-34b3007d373b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "france - paris + rome = italy\n"
          ]
        }
      ],
      "source": [
        "# Word analogies\n",
        "\n",
        "print('')\n",
        "print('france - paris + rome = ' + analogy('paris', 'france', 'rome', word_vectors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIG9R98haSTE"
      },
      "source": [
        "## A word about biases in word vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "_mAQKglTaSTE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e6df6b-5d51-4354-e6fc-1ac9a3871771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "similarity(genius, man) = 0.445\n",
            "similarity(genius, woman) = 0.325\n"
          ]
        }
      ],
      "source": [
        "## A word about biases in word vectors:\n",
        "\n",
        "print('')\n",
        "print('similarity(genius, man) = %.3f' %\n",
        "      cosine(word_vectors['man'], word_vectors['genius']))\n",
        "print('similarity(genius, woman) = %.3f' %\n",
        "      cosine(word_vectors['woman'], word_vectors['genius']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "7IDky3UJaSTF"
      },
      "outputs": [],
      "source": [
        "## Compute the association strength between:\n",
        "##   - a word w\n",
        "##   - two sets of attributes A and B\n",
        "\n",
        "def association_strength(w, A, B, vectors):\n",
        "    '''\n",
        "    Parameters:\n",
        "    w (string): word w\n",
        "    A (list of strings): The words belonging to set A\n",
        "    B (list of strings): The words belonging to set B\n",
        "    vectors (Python dict): {word (string): np-array of word vector}\n",
        "    \n",
        "    Returnrs: \n",
        "    strength (float): the value of the association strength \n",
        "    '''\n",
        "    \n",
        "    strength = 0.0\n",
        "    part_a = 0.0\n",
        "    part_b = 0.0 \n",
        "    \n",
        "    ## FILL CODE\n",
        "    for a in A:\n",
        "      part_a+=cosine(vectors[w],vectors[a])\n",
        "    for b in B:\n",
        "      part_b+=cosine(vectors[w],vectors[b])\n",
        "\n",
        "    strength=(1/len(A))*part_a-(1/len(B))*part_b\n",
        "\n",
        "    return strength"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=['man', 'genius']\n",
        "B=['man', 'cat']\n",
        "w='cats'\n",
        "vectors=word_vectors\n",
        "\n",
        "association_strength(w, A, B, vectors)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koc-2-Hf93oT",
        "outputId": "a2299165-c2f9-4875-8ca1-be3f6861a42b"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.2797361319682652"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2L35dI53Au1g"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weat 1"
      ],
      "metadata": {
        "id": "T9UxpOYUaABo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "Q6YFtSAmaSTF"
      },
      "outputs": [],
      "source": [
        "# ## Perform the word embedding association test between:\n",
        "# ##   - two sets of words X and Y\n",
        "# ##   - two sets of attributes A and B\n",
        "\n",
        "# def weat(X, Y, A, B, vectors):\n",
        "#     '''\n",
        "#     Parameters:\n",
        "#     X (list of strings): The words belonging to set X\n",
        "#     Y (list of strings): The words belonging to set Y\n",
        "#     A (list of strings): The words belonging to set A\n",
        "#     B (list of strings): The words belonging to set B\n",
        "#     vectors (Python dict): {word (string): np-array of word vector}\n",
        "    \n",
        "#     Returns: \n",
        "#     score (float): the value of the group association strength  \n",
        "#     '''\n",
        "    \n",
        "#     score = 0.0\n",
        "#     part_a=0.0\n",
        "#     part_b=0.0\n",
        "#     strength1=0.0\n",
        "#     strength2=0.0\n",
        "#     ## FILL CODE\n",
        "#     for x in X:\n",
        "#       for a in A:\n",
        "#         part_a+=cosine(vectors[x],vectors[a])\n",
        "#       for b in B:\n",
        "#         part_b+=cosine(vectors[x],vectors[a])\n",
        "#       strength1+=(1/len(A))*part_a-(1/len(B))*part_b\n",
        "#     for y in Y:\n",
        "#       for a in A:\n",
        "#         part_a+=cosine(vectors[y],vectors[a])\n",
        "#       for b in B:\n",
        "#         part_b+=cosine(vectors[y],vectors[a])\n",
        "#       strength2+=(1/len(A))*part_a-(1/len(B))*part_b\n",
        "\n",
        "#     score =strength1-strength2\n",
        "#     return score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weat2"
      ],
      "metadata": {
        "id": "OEnrBLMFaEOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weat(X, Y, A, B, vectors):\n",
        "    '''\n",
        "    Parameters:\n",
        "    X (list of strings): The words belonging to set X\n",
        "    Y (list of strings): The words belonging to set Y\n",
        "    A (list of strings): The words belonging to set A\n",
        "    B (list of strings): The words belonging to set B\n",
        "    vectors (Python dict): {word (string): np-array of word vector}\n",
        "    \n",
        "    Returns: \n",
        "    score (float): the value of the group association strength  \n",
        "    '''\n",
        "    \n",
        "    score = 0.0\n",
        "    part_a=0.0\n",
        "    part_b=0.0\n",
        "    strength1=0.0\n",
        "    strength2=0.0\n",
        "    ## FILL CODE\n",
        "    for x in X:\n",
        "      strength1+=association_strength(x, A, B, vectors)\n",
        "    for y in Y:\n",
        "      strength2+=association_strength(y, A, B, vectors)\n",
        "\n",
        "    score =(strength1-strength2)\n",
        "    return score"
      ],
      "metadata": {
        "id": "TDUtKTFvxD5R"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "career = ['executive', 'management', 'professional', 'corporation', \n",
        "          'salary', 'office', 'business', 'career']\n",
        "family = ['home', 'parents', 'children', 'family',\n",
        "          'cousins', 'marriage', 'wedding', 'relatives']\n",
        "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
        "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
        "len(career),len(family),len(male),len(female)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8JYNo4BAId2",
        "outputId": "067c020e-44f7-4e24-9dd5-a6d5bb7cf3d6"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 8, 8, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i76lfmvfAwfa"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "pZgIUgKlaSTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2947ae5-c1dd-46ac-a48b-aaa217c66c71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word embedding association test: 0.847\n"
          ]
        }
      ],
      "source": [
        "## Replicate one of the experiments from:\n",
        "##\n",
        "## Semantics derived automatically from language corpora contain human-like biases\n",
        "## Caliskan, Bryson, Narayanan (2017)\n",
        "\n",
        "career = ['executive', 'management', 'professional', 'corporation', \n",
        "          'salary', 'office', 'business', 'career']\n",
        "family = ['home', 'parents', 'children', 'family',\n",
        "          'cousins', 'marriage', 'wedding', 'relatives']\n",
        "male = ['john', 'paul', 'mike', 'kevin', 'steve', 'greg', 'jeff', 'bill']\n",
        "female = ['amy', 'joan', 'lisa', 'sarah', 'diana', 'kate', 'ann', 'donna']\n",
        "\n",
        "print('')\n",
        "print('Word embedding association test: %.3f' %\n",
        "      weat(career, family, male, female, word_vectors))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2mxGm9MaSTF"
      },
      "source": [
        "## Word translation using word vectors\n",
        "\n",
        "In the following, we will use word vectors in English and French to translate words from English to French. The idea is to learn a linear function that maps English word vectors to their correponding French word vectors. To learn this linear mapping, we will use a small bilingual lexicon, that contains pairs of words in English and French that are translations of each other.\n",
        "\n",
        "The following function will load the small English-French bilingual lexicon:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "ggHZ4mHTaSTF"
      },
      "outputs": [],
      "source": [
        "def load_lexicon(filename):\n",
        "    '''\n",
        "    Parameters:\n",
        "    filename(string): the path of the lexicon\n",
        "    \n",
        "    Returns:\n",
        "    data(list of pairs of string): the bilingual lexicon\n",
        "    '''\n",
        "    fin = io.open(filename, 'r', encoding='utf-8', newline='\\n')\n",
        "    data = []\n",
        "    for line in fin:\n",
        "        a, b = line.rstrip().split(' ')\n",
        "        data.append((a, b))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "6_mByv9iaSTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3561d3-129d-432d-8162-64009631cd47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('the', 'le'), ('the', 'les'), ('the', 'la'), ('and', 'et'), ('was', 'fut')]\n"
          ]
        }
      ],
      "source": [
        "word_vectors_en = load_vectors('wiki.en.vec')\n",
        "word_vectors_fr = load_vectors('wiki.fr.vec')\n",
        "lexicon = load_lexicon(\"lexicon-en-fr.txt\")\n",
        "print(lexicon[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#word_vectors_en[lexicon[1][0]]"
      ],
      "metadata": {
        "id": "K60fO9dy7_Kw"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon[1][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "mY1TUuZF3RER",
        "outputId": "7ed3236a-f1a1-4c2f-e7dd-bfef0645d3da"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lexicon[1][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6ulWRAyo3Ul-",
        "outputId": "d0cb3ca9-b071-41b0-c5f5-4509c1169e44"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'les'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "KNc68Ey1aSTF"
      },
      "outputs": [],
      "source": [
        "# We split the lexicon into a train and validation set\n",
        "train = lexicon[:5000]\n",
        "valid = lexicon[5000:5100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwYnja-IaSTF"
      },
      "source": [
        "The following function will learn the mapping from English to French. The idea is to build two matrices $X_{\\text{en}}$ and $X_{\\text{fr}}$, and to find a mapping $M$ that minimizes $||X_{\\text{en}} W - X_{\\text{fr}} ||_2$. In numpy, this mapping can be obtained by using the `numpy.linalg.lstsq` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "id": "xminrZKWaSTF"
      },
      "outputs": [],
      "source": [
        "def align(word_vectors_en, word_vectors_fr, lexicon):\n",
        "    '''\n",
        "    Parameters:\n",
        "    word_vectors_en(dict: string -> np.array): English word vectors\n",
        "    word_vectors_en(dict: string -> np.array): French word vectors\n",
        "    lexicon(list of pairs of string): bilingual training lexicon\n",
        "    \n",
        "    Returns\n",
        "    mapping(np.array): the mapping from English to French vectors\n",
        "    '''\n",
        "    x_en, x_fr = [], []\n",
        "    \n",
        "    ## FILL CODE\n",
        "    for word in lexicon:\n",
        "      x_en.append(word_vectors_en[word[0]])\n",
        "      x_fr.append(word_vectors_fr[word[1]])\n",
        "\n",
        "    X_en=np.array(x_en)\n",
        "    X_fr=np.array(x_fr)\n",
        "\n",
        "    M=np.linalg.lstsq(X_en,X_fr)[0]\n",
        "    \n",
        "    return M"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "2sW5ovAbaSTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a24fb541-ea31-49e8-944a-854b72f48ece"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "mapping = align(word_vectors_en, word_vectors_fr, lexicon)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A=mapping\n",
        "A.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gALeD3Q4A4qB",
        "outputId": "f3aaef76-d41f-4f8c-dfae-2562a803981e"
      },
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(300, 300)"
            ]
          },
          "metadata": {},
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aalJnxInaSTG"
      },
      "source": [
        "Given a mapping, a set of word English word vector and French word vectors, the next function will translate the English word to French. To do so, we apply the mapping on the English word, and retrieve the nearest neighbor of the obtained vector in the set of French word vectors. The translation is then the corresponding French word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "pWQRP-nTaSTG"
      },
      "outputs": [],
      "source": [
        "def translate(word, word_vectors_en, word_vectors_fr, mapping):\n",
        "    '''\n",
        "    Parameters:\n",
        "    word(string): an English word\n",
        "    word_vectors_en(dict: string -> np.array): English word vectors\n",
        "    word_vectors_en(dict: string -> np.array): French word vectors\n",
        "    mapping(np.array): the mapping from English to French vectors\n",
        "    \n",
        "    Returns\n",
        "    A string containing the translation of the English word\n",
        "    '''\n",
        "    \n",
        "    ## FILL CODE\n",
        "    enwor=word_vectors_en[word]\n",
        "    frwor=enwor@mapping\n",
        "    nn=nearest_neighbor(frwor, word_vectors_fr, exclude_words = [])#nearest neighbour\n",
        "    \n",
        "\n",
        "\n",
        "    return nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {
        "id": "_3BuxiJtaSTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa1bc43-0cb7-4504-f29d-30b905cf014d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "monde\n",
            "machine\n",
            "apprentissage\n"
          ]
        }
      ],
      "source": [
        "print(translate(\"world\", word_vectors_en, word_vectors_fr, mapping))\n",
        "print(translate(\"machine\", word_vectors_en, word_vectors_fr, mapping))\n",
        "print(translate(\"learning\", word_vectors_en, word_vectors_fr, mapping))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoppZVCNaSTG"
      },
      "source": [
        "Finally, let's implement a function to evaluate this method on the validation lexicon:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "bGWQqQZpaSTG"
      },
      "outputs": [],
      "source": [
        "def evaluate(valid, word_vectors_en, word_vectors_fr, mapping):\n",
        "    '''\n",
        "    Parameters:\n",
        "    valid(a list of pairs of string): the validation lexicon\n",
        "    word_vectors_en(dict: string -> np.array): English word vectors\n",
        "    word_vectors_en(dict: string -> np.array): French word vectors\n",
        "    mapping(np.array): the mapping from English to French vectors\n",
        "    \n",
        "    Returns\n",
        "    Accuracy(float): the accuracy on the validation lexicon\n",
        "    '''\n",
        "    acc, n = 0.0, len(valid)\n",
        "    \n",
        "    ## FILL CODE\n",
        "    for en,fr in valid:\n",
        "      trans=translate(en, word_vectors_en, word_vectors_fr, mapping)\n",
        "      if trans==fr:\n",
        "        acc+=1\n",
        "    acc=acc*100/n\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {
        "id": "efXbUGGhaSTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e3c5129-e835-4c48-dc49-ddbeee34611c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64.0"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ],
      "source": [
        "evaluate(valid, word_vectors_en, word_vectors_fr, mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEc6gDiDaSTG"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2+"
    },
    "colab": {
      "name": "AlbertAgisha_Lab2_intro_to_wordvectors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}